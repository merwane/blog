---
title: "Ether as a computation unit"
date: 2021-04-24T14:18:37+02:00
draft: false
---

Because running software is never free in terms of resources, Ether could be adopted as a primary unit to quantifiably measure computation costs. Instead of relying on system requirements as a metric for running software, we could adopt Ether as a unique indicator of whether a computation can be performed. Essentially, instead of having the recommended CPU and a certain amount of memory as an indicator, we could simply have a _float_. Imagine having a cake sold at a bakery and priced in ingredients and steps. This cake costs 2 eggs, 4 cups of flour, 40 minutes of baking, etc. This is how determining if we can run software is done nowadays.

**Running Cyberpunk 2077 vs. getting DAI on Uniswap:**

![Cyberpunk](/cybermask.png)

Since I believe [most](https://mightyapp.com/) [heavy](https://shadow.tech/) [computation](https://venturebeat.com/2021/02/18/replit-raises-20-million-for-collaborative-browser-based-coding/) is going to end up being done remotely, this could be a good way to know what you'll be paying for. Maybe DApps were just a kickstart to Ether being used as a computation unit.