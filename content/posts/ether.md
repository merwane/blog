---
title: "Ether as a computation unit"
date: 2021-04-24T14:18:37+02:00
draft: false
---

The Bitcoin community dreams of [Hyperbitcoinization](https://nakamotoinstitute.org/mempool/hyperbitcoinization/), and believes the endgame of bitcoin (or satoshis, a smaller denomination of it) is to become a major unit of account. Their idea is to have things commercially [priced in bitcoin](https://nakamotoinstitute.org/mempool/bitcoin-is-the-best-unit-of-account/) rather than the local currency. I don't know if the Ethereum community shares the same vision for Ether, but this isn't necessarily what is needed, or the best we can get out of the cryptocurrency.

Because running software is never free in terms of resources, Ether could be adopted as a primary unit to quantifiably measure computation costs. Instead of relying on system requirements as a metric for running software, we could adopt Ether as a unique indicator of whether a computation can be performed. Essentially, instead of having the recommended CPU and a certain amount of memory as an indicator, we could simply have a _float_. Imagine having a cake sold at a bakery and priced in ingredients and steps. This cake costs 2 eggs, 4 cups of flour, 40 minutes of baking, etc. This is how determining if we can run software is done nowadays.

**Running Cyberpunk 2077 vs. getting DAI on Uniswap:**

![Cyberpunk](/cybermask.png)

Since I believe [most](https://mightyapp.com/) [heavy](https://shadow.tech/) [computation](https://venturebeat.com/2021/02/18/replit-raises-20-million-for-collaborative-browser-based-coding/) is going to end up being done remotely, this could be a good way to know what you'll be paying for. Maybe DApps were just a kickstart to Ether being used as a computation unit.